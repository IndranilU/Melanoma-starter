{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jpeg-melanoma-256x256',\n",
       " 'siim-isic-melanoma-classification',\n",
       " 'jpeg-melanoma-384x384',\n",
       " 'melanoma-hairs']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sample_submission.csv', 'test.csv', 'train.csv', 'train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../input/jpeg-melanoma-384x384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\r\n",
      "Collecting torchtoolbox\r\n",
      "  Downloading torchtoolbox-0.1.4.1-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.1)\r\n",
      "Collecting lmdb\r\n",
      "  Downloading lmdb-0.98.tar.gz (869 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 869 kB 7.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.3.0.36)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.23.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, lmdb\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=29b7450b4d4d4c9223817f9aa51cf2d7ab948397d6db41484501ccf5de9b40fe\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\r\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=273151 sha256=ec8b590f47436e9fb7a67ea23841d713d1f9881a9565998f6775f937c15426cc\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\r\n",
      "Successfully built efficientnet-pytorch lmdb\r\n",
      "Installing collected packages: efficientnet-pytorch, lmdb, torchtoolbox\r\n",
      "Successfully installed efficientnet-pytorch-0.6.3 lmdb-0.98 torchtoolbox-0.1.4.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch torchtoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchtoolbox.transform as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### still difficult to reproduce ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, imfolder: str, train: bool = True, transforms = None, meta_features = None):\n",
    "        \"\"\"\n",
    "        Class initialization\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with data description\n",
    "            imfolder (str): folder with images\n",
    "            train (bool): flag of whether a training dataset is being initialized or testing one\n",
    "            transforms: image transformation method to be applied\n",
    "            meta_features (list): list of features with meta information, such as sex and age\n",
    "            \n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.imfolder = imfolder\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        self.meta_features = meta_features\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        im_path = os.path.join(self.imfolder, self.df.iloc[index]['image_name'] + '.jpg')\n",
    "        x = cv2.imread(im_path)\n",
    "        meta = np.array(self.df.iloc[index][self.meta_features].values, dtype=np.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "            \n",
    "        if self.train:\n",
    "            y = self.df.iloc[index]['target']\n",
    "            return (x, meta), y\n",
    "        else:\n",
    "            return (x, meta)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, arch, n_meta_features: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.arch = arch\n",
    "        if 'ResNet' in str(arch.__class__):\n",
    "            self.arch.fc = nn.Linear(in_features=512, out_features=500, bias=True)\n",
    "        if 'EfficientNet' in str(arch.__class__):\n",
    "            self.arch._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n",
    "        self.meta = nn.Sequential(nn.Linear(n_meta_features, 500),\n",
    "                                  nn.BatchNorm1d(500),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2),\n",
    "                                  nn.Linear(500, 250),  # FC layer output will have 250 features\n",
    "                                  nn.BatchNorm1d(250),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        self.ouput = nn.Linear(500 + 250, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n",
    "        Which applies sigmoid for us when calculating a loss\n",
    "        \"\"\"\n",
    "        x, meta = inputs\n",
    "        cnn_features = self.arch(x)\n",
    "        meta_features = self.meta(meta)\n",
    "        features = torch.cat((cnn_features, meta_features), dim=1)\n",
    "        output = self.ouput(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedHairAugmentation:\n",
    "    \"\"\"\n",
    "    Impose an image of a hair to the target image\n",
    "\n",
    "    Args:\n",
    "        hairs (int): maximum number of hairs to impose\n",
    "        hairs_folder (str): path to the folder with hairs images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hairs: int = 5, hairs_folder: str = \"\"):\n",
    "        self.hairs = hairs\n",
    "        self.hairs_folder = hairs_folder\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to draw hairs on.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Image with drawn hairs.\n",
    "        \"\"\"\n",
    "        n_hairs = random.randint(0, self.hairs)\n",
    "        \n",
    "        if not n_hairs:\n",
    "            return img\n",
    "        \n",
    "        height, width, _ = img.shape  # target image width and height\n",
    "        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n",
    "        \n",
    "        for _ in range(n_hairs):\n",
    "            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n",
    "            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n",
    "            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n",
    "\n",
    "            h_height, h_width, _ = hair.shape  # hair image width and height\n",
    "            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n",
    "            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n",
    "            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n",
    "\n",
    "            # Creating a mask and inverse mask\n",
    "            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n",
    "            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
    "            mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "            # Now black-out the area of hair in ROI\n",
    "            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
    "\n",
    "            # Take only region of hair from hair image.\n",
    "            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n",
    "\n",
    "            # Put hair in ROI and modify the target image\n",
    "            dst = cv2.add(img_bg, hair_fg)\n",
    "\n",
    "            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n",
    "                \n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawHair:\n",
    "    \"\"\"\n",
    "    Draw a random number of pseudo hairs\n",
    "\n",
    "    Args:\n",
    "        hairs (int): maximum number of hairs to draw\n",
    "        width (tuple): possible width of the hair in pixels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hairs:int = 4, width:tuple = (1, 2)):\n",
    "        self.hairs = hairs\n",
    "        self.width = width\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to draw hairs on.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Image with drawn hairs.\n",
    "        \"\"\"\n",
    "        if not self.hairs:\n",
    "            return img\n",
    "        \n",
    "        width, height, _ = img.shape\n",
    "        \n",
    "        for _ in range(random.randint(0, self.hairs)):\n",
    "            # The origin point of the line will always be at the top half of the image\n",
    "            origin = (random.randint(0, width), random.randint(0, height // 2))\n",
    "            # The end of the line \n",
    "            end = (random.randint(0, width), random.randint(0, height))\n",
    "            color = (0, 0, 0)  # color of the hair. Black.\n",
    "            cv2.line(img, origin, end, color, random.randint(self.width[0], self.width[1]))\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(hairs={self.hairs}, width={self.width})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    AdvancedHairAugmentation(hairs_folder='/kaggle/input/melanoma-hairs'),\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    #Microscope(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/checkpoints/efficientnet-b0-355c32eb.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15703f174a1e43aa95268df0e1bad51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21388428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "arch = EfficientNet.from_pretrained('efficientnet-b0')  # Going to use efficientnet-b1 NN architecture\n",
    "# skf = StratifiedKFold(n_splits=5, random_state=999, shuffle=True)\n",
    "skf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/jpeg-melanoma-256x256/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/jpeg-melanoma-256x256/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33126, 11), (10982, 7))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>tfrecord</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1872</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1872</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
       "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
       "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
       "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
       "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
       "\n",
       "  diagnosis benign_malignant  target  tfrecord  width  height  \n",
       "0   unknown           benign       0         0   6000    4000  \n",
       "1   unknown           benign       0         0   6000    4000  \n",
       "2     nevus           benign       0         6   1872    1053  \n",
       "3   unknown           benign       0         0   1872    1053  \n",
       "4   unknown           benign       0        11   6000    4000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('/kaggle/input/jpeg-melanoma-256x256/train.csv')\n",
    "train_df['fold'] = tmp['tfrecord']\n",
    "del tmp\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits = StratifiedKFold(n_splits=nfolds, random_state=2020, shuffle=True)\n",
    "#splits = list(splits.split(train_df,train_df.target))\n",
    "#folds_splits = np.zeros(len(train_df)).astype(np.int)\n",
    "#for i in range(nfolds): folds_splits[splits[i][1]] = i\n",
    "#train_df['split'] = folds_splits\n",
    "#train_df['fold'] = train_df['tfrecord']\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 12    2198\n",
       " 2     2193\n",
       " 13    2186\n",
       " 1     2185\n",
       " 3     2182\n",
       " 0     2182\n",
       " 9     2178\n",
       " 8     2177\n",
       " 11    2176\n",
       " 6     2175\n",
       " 14    2174\n",
       " 10    2174\n",
       " 7     2174\n",
       " 5     2171\n",
       " 4     2167\n",
       "-1      434\n",
       "Name: tfrecord, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"tfrecord\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.tfrecord != -1]\n",
    "#train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.groupby(['split','target'])['target'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of anatom_site_general_challenge feature\n",
    "concat = pd.concat([train_df['anatom_site_general_challenge'], test_df['anatom_site_general_challenge']], ignore_index=True)\n",
    "dummies = pd.get_dummies(concat, dummy_na=True, dtype=np.uint8, prefix='site')\n",
    "train_df = pd.concat([train_df, dummies.iloc[:train_df.shape[0]]], axis=1)\n",
    "test_df = pd.concat([test_df, dummies.iloc[train_df.shape[0]:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Sex features\n",
    "train_df['sex'] = train_df['sex'].map({'male': 1, 'female': 0})\n",
    "test_df['sex'] = test_df['sex'].map({'male': 1, 'female': 0})\n",
    "train_df['sex'] = train_df['sex'].fillna(-1)\n",
    "test_df['sex'] = test_df['sex'].fillna(-1)\n",
    "\n",
    "# Age features\n",
    "train_df['age_approx'] /= train_df['age_approx'].max()\n",
    "test_df['age_approx'] /= test_df['age_approx'].max()\n",
    "train_df['age_approx'] = train_df['age_approx'].fillna(0)\n",
    "test_df['age_approx'] = test_df['age_approx'].fillna(0)\n",
    "\n",
    "train_df['patient_id'] = train_df['patient_id'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['sex', 'age_approx'] + [col for col in train_df.columns if 'site_' in col]\n",
    "meta_features.remove('anatom_site_general_challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MelanomaDataset(df=test_df,\n",
    "                       imfolder='/kaggle/input/jpeg-melanoma-256x256/test/', \n",
    "                       train=False,\n",
    "                       transforms=train_transform,  # For TTA\n",
    "                       meta_features=meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Fold 1 ====================\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 001: | Loss: 70.322 | Train acc: 0.980 | Val acc: 0.982 | Val roc_auc: 0.820 | Training time: 0:05:32\n",
      "==================== Fold 2 ====================\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 001: | Loss: 70.142 | Train acc: 0.980 | Val acc: 0.982 | Val roc_auc: 0.844 | Training time: 0:05:15\n",
      "==================== Fold 3 ====================\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 001: | Loss: 70.572 | Train acc: 0.981 | Val acc: 0.982 | Val roc_auc: 0.816 | Training time: 0:05:17\n",
      "==================== Fold 4 ====================\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 001: | Loss: 70.475 | Train acc: 0.982 | Val acc: 0.982 | Val roc_auc: 0.786 | Training time: 0:05:16\n",
      "==================== Fold 5 ====================\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch 001: | Loss: 72.305 | Train acc: 0.981 | Val acc: 0.982 | Val roc_auc: 0.848 | Training time: 0:05:15\n"
     ]
    }
   ],
   "source": [
    "epochs = 1  # Number of epochs to run\n",
    "es_patience = 3  # Early Stopping patience - for how many epochs with no improvements to wait, would be useful for more epochs\n",
    "TTA = 3 # Test Time Augmentation rounds\n",
    "\n",
    "oof = np.zeros((len(train_df), 1))  # Out Of Fold predictions\n",
    "preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)  # Predictions for test test\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "for fold,(idxT, idxV) in enumerate(skf.split(np.arange(15)), 1):\n",
    "    print('=' * 20, 'Fold', fold, '=' * 20)\n",
    "    \n",
    "    train_idx = train_df.loc[train_df['fold'].isin(idxT)].index\n",
    "    val_idx = train_df.loc[train_df['fold'].isin(idxV)].index\n",
    "    \n",
    "    \n",
    "    model_path = f'model_{fold}.pth'  # Path and filename to save model to\n",
    "    best_val = 0  # Best validation score within this fold\n",
    "    patience = es_patience  # Current patience counter\n",
    "    arch = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    model = Net(arch=arch, n_meta_features=len(meta_features))  # New model for each fold\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train = MelanomaDataset(df=train_df.iloc[train_idx].reset_index(drop=True), \n",
    "                            imfolder='/kaggle/input/jpeg-melanoma-256x256/train/', \n",
    "                            train=True, \n",
    "                            transforms=train_transform,\n",
    "                            meta_features=meta_features)\n",
    "    val = MelanomaDataset(df=train_df.iloc[val_idx].reset_index(drop=True), \n",
    "                            imfolder='/kaggle/input/jpeg-melanoma-256x256/train/', \n",
    "                            train=True, \n",
    "                            transforms=test_transform,\n",
    "                            meta_features=meta_features)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train, batch_size=32, shuffle=True, num_workers=2, drop_last=True)\n",
    "    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(dataset=test, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n",
    "            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "            optim.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
    "            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n",
    "            epoch_loss += loss.item()\n",
    "        train_acc = correct / len(train_idx)\n",
    "        \n",
    "        model.eval()  # switch model to the evaluation mode\n",
    "        val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
    "            # Predicting on validation set\n",
    "            for j, (x_val, y_val) in enumerate(val_loader):\n",
    "                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "                z_val = model(x_val)\n",
    "                val_pred = torch.sigmoid(z_val)\n",
    "                val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
    "            val_acc = accuracy_score(train_df.iloc[val_idx]['target'].values, torch.round(val_preds.cpu()))\n",
    "            val_roc = roc_auc_score(train_df.iloc[val_idx]['target'].values, val_preds.cpu())\n",
    "            \n",
    "            print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n",
    "            epoch + 1, \n",
    "            epoch_loss, \n",
    "            train_acc, \n",
    "            val_acc, \n",
    "            val_roc, \n",
    "            str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n",
    "            \n",
    "            scheduler.step(val_roc)\n",
    "                \n",
    "            if val_roc >= best_val:\n",
    "                best_val = val_roc\n",
    "                patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
    "                torch.save(model, model_path)  # Saving current best model\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n",
    "                    break\n",
    "                \n",
    "    model = torch.load(model_path)  # Loading best model of this fold\n",
    "    model.eval()  # switch model to the evaluation mode\n",
    "    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Predicting on validation set once again to obtain data for OOF\n",
    "        for j, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "            z_val = model(x_val)\n",
    "            val_pred = torch.sigmoid(z_val)\n",
    "            val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
    "        oof[val_idx] = val_preds.cpu().numpy()\n",
    "        \n",
    "        # Predicting on test set\n",
    "        for _ in range(TTA):\n",
    "            for i, x_test in enumerate(test_loader):\n",
    "                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n",
    "                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n",
    "                z_test = model(x_test)\n",
    "                z_test = torch.sigmoid(z_test)\n",
    "                preds[i*test_loader.batch_size:i*test_loader.batch_size + x_test[0].shape[0]] += z_test\n",
    "        preds /= TTA\n",
    "        \n",
    "    del train, val, train_loader, val_loader, x, y, x_val, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "preds /= skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF: 0.805\n"
     ]
    }
   ],
   "source": [
    "print('OOF: {:.3f}'.format(roc_auc_score(train_df['target'], oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(oof.reshape(-1,)).to_csv('oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "sub['target'] = preds.cpu().numpy().reshape(-1,)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sub[\"target\"] , bins = 100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "15703f174a1e43aa95268df0e1bad51d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82299f1d41074a369cede85d9f2ef6f3",
        "IPY_MODEL_b955707df0494c7b8ffd777e5200703e"
       ],
       "layout": "IPY_MODEL_d3d83a456e254331899adbe015b77cd3"
      }
     },
     "2111ea67430248988ee8208c319eb8f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "331458e068c9442b9e4586705ddc2030": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41cd4d2db0ff490da3c4176ad9842681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "82299f1d41074a369cede85d9f2ef6f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2111ea67430248988ee8208c319eb8f3",
       "max": 21388428.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a2079e02983e44bab6688babfeaeda2b",
       "value": 21388428.0
      }
     },
     "a2079e02983e44bab6688babfeaeda2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b955707df0494c7b8ffd777e5200703e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_331458e068c9442b9e4586705ddc2030",
       "placeholder": "​",
       "style": "IPY_MODEL_41cd4d2db0ff490da3c4176ad9842681",
       "value": " 20.4M/20.4M [00:02&lt;00:00, 8.48MB/s]"
      }
     },
     "d3d83a456e254331899adbe015b77cd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
